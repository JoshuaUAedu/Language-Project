{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069253ac",
   "metadata": {},
   "source": [
    "## 0 - Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a20e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Audio handling\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# Audio Recording\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Transcription\n",
    "from jiwer import wer\n",
    "\n",
    "# Misc\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2b7a1",
   "metadata": {},
   "source": [
    "## 1 - Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9e00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training portion of each language - takes a couple minutes\n",
    "fleurs_en = load_dataset(\"google/fleurs\", \"en_us\", split=\"train\",trust_remote_code = True)\n",
    "fleurs_es = load_dataset(\"google/fleurs\", \"es_419\", split=\"train\", trust_remote_code = True)\n",
    "fleurs_ja = load_dataset(\"google/fleurs\", \"ja_jp\", split=\"train\", trust_remote_code = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d5f25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 903,\n",
       " 'num_samples': 108800,\n",
       " 'path': 'C:\\\\Users\\\\jking36\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\00f4c8069bdbe2061746b64297f5f7dd1af5f4ac9c3f84e1cc27006199d80190\\\\10004088536354799741.wav',\n",
       " 'audio': {'path': 'train/10004088536354799741.wav',\n",
       "  'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         -3.15904617e-06, -3.03983688e-06, -3.27825546e-06], shape=(108800,)),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'a tornado is a spinning column of very low-pressure air which sucks the surrounding air inward and upward',\n",
       " 'raw_transcription': 'A tornado is a spinning column of very low-pressure air, which sucks the surrounding air inward and upward.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 19,\n",
       " 'language': 'English',\n",
       " 'lang_group_id': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See outputs\n",
    "sample_en = fleurs_en[0]\n",
    "sample_en.keys()\n",
    "\n",
    "sample_en\n",
    "\n",
    "#key_features = ['audio' , 'transcription', 'language']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554c60e",
   "metadata": {},
   "source": [
    "## 2 - Obtain ASR Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf39259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1 \n",
    "\n",
    "asr = pipeline(\n",
    "    'automatic-speech-recognition',\n",
    "    model = 'openai/whisper-small',\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# Or Directly import model\n",
    "\n",
    "model_name = 'openai/whisper-small'\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "whisper.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f34678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text:  A tornado is a spinning column of very low pressure air, which sucks the surrounding air inward and upward. - Predicted Language: None\n",
      "True ENG Text: a tornado is a spinning column of very low-pressure air which sucks the surrounding air inward and upward - True Language: English\n"
     ]
    }
   ],
   "source": [
    "#Use English sample to test asr\n",
    "\n",
    "audio = sample_en['audio']['array']\n",
    "\n",
    "testing = asr(audio)\n",
    "\n",
    "print(f'Predicted Text: {testing[\"text\"]} - Predicted Language: {testing.get(\"language\")}')\n",
    "print(f'True ENG Text: {sample_en[\"transcription\"]} - True Language: {sample_en[\"language\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b673b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Language Detection\n",
    "def detect_language(dataset,n=50):\n",
    "    true = 0\n",
    "    for i in range(n):\n",
    "        sample = dataset[i]\n",
    "        prediction = asr(sample['audio']['array'])\n",
    "        if prediction.get('language') == sample['language']:\n",
    "            true += 1\n",
    "        print(prediction.get('language'))\n",
    "        print(sample['language'])\n",
    "    return true/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6188c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "English\n",
      "English Language Detection Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('English Language Detection Accuracy: ', detect_language(fleurs_en,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfa856",
   "metadata": {},
   "source": [
    "## 3 - Record Audio for Model\n",
    "    Function to record audio clip for Language Detection and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7c283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording in process...\n",
      "Audio Recording Saved --> \"recording.wav\"\n"
     ]
    }
   ],
   "source": [
    "#### RECORD AUDIO FILE .wav\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 30 #seconds\n",
    "OUTPUT = 'recording.wav'\n",
    "\n",
    "print('Recording in process...')\n",
    "\n",
    "audio = sd.rec(\n",
    "    int(DURATION * SAMPLE_RATE),\n",
    "    samplerate = SAMPLE_RATE,\n",
    "    channels = 1,\n",
    "    dtype = np.float32\n",
    ")\n",
    "\n",
    "sd.wait()\n",
    "\n",
    "write(OUTPUT,SAMPLE_RATE,audio)\n",
    "print('Audio Recording Saved --> \"recording.wav\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540b5c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription of Recording audio:  Yeah, so it's almost New Year's Eve and I am practicing with my language detection transcription. Let's also see if it can do a mix of languages. So I'm gonna say Ego dekimasu ka arigatou gozaimasu. And we're gonna see if that works. So three, two, one, see ya.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe Audio Recording\n",
    "# --> RUNTIME = 13 sec\n",
    "recording, sr = librosa.load('recording.wav',sr = 16000)\n",
    "transcribed = asr(recording)\n",
    "print(f\"Transcription of Recording audio: {transcribed['text']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
